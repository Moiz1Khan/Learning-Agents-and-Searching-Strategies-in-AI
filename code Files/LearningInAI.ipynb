{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f62833a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M O I Z\\AppData\\Local\\Temp\\ipykernel_19584\\306521753.py:91: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy with sigmoid activation function: 100.00%\n",
      "\n",
      "Accuracy with relu activation function: 56.67%\n",
      "\n",
      "Accuracy with tanh activation function: 56.67%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "Gradient Descent Result:\n",
      "Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Data manipulation and analysis\n",
    "import numpy as np # Mathematical functions and computing\n",
    "from sklearn.model_selection import train_test_split # Split arrays\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score # Evaluation metrics\n",
    "from sklearn.preprocessing import StandardScaler # Standardization\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, learning_rate, n_iterations=2000, n_hidden_units=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.weights_input_hidden = None\n",
    "        self.weights_hidden_output = None\n",
    "\n",
    "    def sigmoid(self, x): # Activation function 1\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x): # Activation function 2\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def tanh(self, x): # Activation function 3\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def sigmoid_derivative(self, x): # Derivative for sigmoid\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "        \n",
    "    def fit(self, X, y, activation_function='sigmoid'):  # Train the neural network\n",
    "        np.random.seed(42)\n",
    "        self.weights_input_hidden = np.random.uniform(low=-0.1, high=0.1, size=(X.shape[1], self.n_hidden_units))\n",
    "        self.weights_hidden_output = np.random.uniform(low=-0.1, high=0.1, size=(self.n_hidden_units, y.shape[1]))\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            if activation_function == 'sigmoid':\n",
    "                activation_function = self.sigmoid\n",
    "                activation_derivative = self.sigmoid_derivative\n",
    "            elif activation_function == 'relu':\n",
    "                activation_function = self.relu\n",
    "                activation_derivative = self.relu_derivative\n",
    "            elif activation_function == 'tanh':\n",
    "                activation_function = self.tanh\n",
    "                activation_derivative = self.tanh_derivative\n",
    "            \n",
    "            hidden_layer_input = np.dot(X, self.weights_input_hidden) # Hidden layer input and output\n",
    "            hidden_layer_output = activation_function(hidden_layer_input)\n",
    "\n",
    "            output_layer_input = np.dot(hidden_layer_output, self.weights_hidden_output)\n",
    "            output_layer_output = self.sigmoid(output_layer_input)\n",
    "\n",
    "            error = y - output_layer_output # Compute error\n",
    "            d_output = error * self.sigmoid_derivative(output_layer_output)\n",
    "\n",
    "            error_hidden_layer = d_output.dot(self.weights_hidden_output.T)\n",
    "            d_hidden_layer = error_hidden_layer * activation_derivative(hidden_layer_output)\n",
    "\n",
    "            self.weights_hidden_output += hidden_layer_output.T.dot(d_output) * self.learning_rate\n",
    "            self.weights_input_hidden += X.T.dot(d_hidden_layer) * self.learning_rate\n",
    "\n",
    "    def predict(self, X): # Function to make predictions\n",
    "        hidden_layer_input = np.dot(X, self.weights_input_hidden)\n",
    "        hidden_layer_output = self.sigmoid(hidden_layer_input)\n",
    "\n",
    "        output_layer_input = np.dot(hidden_layer_output, self.weights_hidden_output)\n",
    "        output_layer_output = self.sigmoid(output_layer_input)\n",
    "\n",
    "        return np.argmax(output_layer_output, axis=1)\n",
    "\n",
    "\n",
    "# Define the Gradient Descent Delta Rule algorithm\n",
    "class GradientDescentDelta:\n",
    "    def __init__(self, learning_rate, n_iterations=2000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.uniform(low=-0.1, high=0.1, size=X.shape[1])  # Small random initial weights around zero\n",
    "        for _ in range(self.n_iterations): # Compute error, weights\n",
    "            errors = y - self.predict(X)\n",
    "            update = self.learning_rate * np.dot(X.T, errors)  # Update based on the transpose of X\n",
    "            self.weights += update\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.sigmoid(np.dot(X, self.weights)) >= 0.5, 1, 0)\n",
    "\n",
    "    def accuracy(self, X, y): # Tells the accuracy of gradient rule\n",
    "        predictions = self.predict(X)\n",
    "        correct = np.sum(predictions == y)\n",
    "        return correct / len(y)\n",
    "\n",
    "\n",
    "# Load the Iris dataset from iris.data\n",
    "iris_df = pd.read_csv('iris.data', header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "# Convert species names to numerical labels\n",
    "iris_df['species'] = iris_df['species'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "y = iris_df['species'].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20 ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ask for user input\n",
    "learning_rate = 0.2  # Fixed learning rate for the example\n",
    "\n",
    "# Create an instance of GradientDescentDelta and fit the data\n",
    "gd = GradientDescentDelta(learning_rate)\n",
    "gd.fit(X_train, y_train)\n",
    "gd_accuracy = gd.accuracy(X_test, y_test)\n",
    "\n",
    "# Evaluate MLP model performance on testing set for each activation function\n",
    "activation_functions = ['sigmoid', 'relu', 'tanh']\n",
    "best_accuracy = 0\n",
    "best_predictions = None\n",
    "\n",
    "for activation_function in activation_functions:\n",
    "    mlp_model = MLP(learning_rate)\n",
    "    mlp_model.fit(X_train, np.eye(3)[y_train], activation_function)\n",
    "    mlp_predictions = mlp_model.predict(X_test)\n",
    "    mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "    if mlp_accuracy > best_accuracy:\n",
    "        best_accuracy = mlp_accuracy\n",
    "        best_predictions = mlp_predictions\n",
    "    print(f\"\\nAccuracy with {activation_function} activation function: {mlp_accuracy:.2%}\")\n",
    "\n",
    "# Calculate confusion matrix and classification report for the best MLP model\n",
    "conf_matrix = confusion_matrix(y_test, best_predictions)\n",
    "class_report = classification_report(y_test, best_predictions, zero_division=1)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\nGradient Descent Result:\")\n",
    "print(f\"Accuracy: {gd_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c336fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369eb19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
